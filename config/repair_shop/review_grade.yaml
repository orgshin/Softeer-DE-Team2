s3:
  aws_conn_id: conn_s3            # Airflow Conn ID (login=AKIA..., password=SECRET)
  base_bucket: team2-html-out         # 네 버킷명만 (prefix는 스크립트에서 고정 경로 사용)
  standards_key: standards/car_repair_shop_standards.csv

postgres:
  pg_conn_id: conn_postgres             # Airflow Conn ID (host, port, schema, login, password)
  table_name: shop_review_with_grade
  write_mode: overwrite               # or append
  sslmode: require

spark:
  transformers_cache: /opt/airflow/.cache/hf
  executor_cores: 2
  executor_memory: 4g
  driver_memory: 6g
  num_executors: 2
  extra_conf:                          # 필요 시 spark conf 추가
    spark.sql.shuffle.partitions: "200"
